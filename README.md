# MessagesCompress
Analysis of compression algorithms. Data - a lot of small messages (~100 bytes long)

results.txt - результаты полученные на базе случайных публичных твитов (ее можно сказать по ссылке https://www.dropbox.com/s/uifh1nrj512phlh/random-twits-ru.txt?dl=0).

### Постановка задачи:
Нужно написать алгоритм, который сжимает которкие сообщения. Вначале ему дается на вход вся база сообщений, на этом этапе алгоритм имеет право предподсчатать какие-нибудь статистики и сохранить их в память.

После этого алгоритму последовательно даются отдельные сообщения, которые он должен сжимать и расжимать пользуясь только предподсчитанными данными. 

В качестве меры эффективность алгоритма считаем величину compress_ratio = (total_additional_size + total_compressed_messages_size) / total_messages_size, где 
* total_additional_size - общее количество памяти, которое алгоритм потратил на хранение дополнительных статистик
* total_compressed_messages_size - суммарный размер сообщений в сжатом виде
* total_messages_size - суммарный размер исходных сообщений

Чем меньше данная величина тем лучше. Если величина равна 1, то сообщения сосвем не сжаты.

### Данные
В качестве исходных данных был взят набор публичных случайных русскоязычных твиттов, который получил с помощью twitter Streaming API. Суммарный размер сообщений 248 мегабайт. Общее количество сообщений - 2089378. Средняя длина - 124 символа (все хранится в кодировке cp1251, поэтому некоторые символы могут занимать больше 1 байта). 

### Алгоритмы

#### Empty
Ничего не делает

#### One letter huffman
Считаем вероятность встретить каждую букву, строим на этих вероятностях дерево Хаффмана.

#### LZW
* https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%9B%D0%B5%D0%BC%D0%BF%D0%B5%D0%BB%D1%8F_%E2%80%94_%D0%97%D0%B8%D0%B2%D0%B0_%E2%80%94_%D0%92%D0%B5%D0%BB%D1%87%D0%B0
* Используется 16-битная таблица.
* При переполнении таблицы из нее удаляются слова, которые последний раз встречались раньше всего (принцип LRU)

#### Words huffman
* Разбиваем все симолы на две группы - те, которые встрчаются в словах (большие и маленькие буквы) и все остальные (знаки припенания, цифры, ...).
* Сообщение разбиваем на "слова" (которые состоят из символов первой группы) и "не слова" (состоят из символов второй группы). Понятно, что "слова" и "не слова" в сообщении чередуются. Отдельно сжимаем "слова" и "не слова". 
* Для каждой группы считаем частоту встречаемости слов, а потом строим для них дерево Хаффмана. Чтобы словрь не получился слишком большим, выкидывает те слова, которые встрчаются меньше 3 раз (константа должна зависеть от исходных данных). Добавляем слово-исключение, веростность которого равна сумме вероятностей всех слов, которые были удалены из словаря. В случае необходимости кодирования слова, которого нет в словаре, записываем слово-исключение, а потом текущие слово побайтово совпадающее с исходным.

#### Words huffman, bytes huffman
Все тоже самое, что и "Words huffman", только для слов, которые не попали в словарь, применяем метод кодирования "One letter huffman"

#### Words huffman, bytes 2 huffman
Все тоже самое, что и "Words huffman", только для слов, которые не попали в словарь, применяем следующий метод сжатия. Предподсчитывем вероятность символа в зависимости от предыдущего (необходимо 256 * 256 * 4 байт для хранения). После этого строим 256 различных деревьев Хаффмана для каждого возможного предыдущего символа. 

### Результаты

|Алгоритм|Compression ratio|Размер дополнительной информации (в байтах)|
|-|-|-|
|One letter huffman | 0.7278|1024|
|LZW | 0.5837|327680|
|Words huffman | 0.3755|4792892|
|Words huffman, bytes huffman | 0.3634|3912623|
|Words huffman, bytes 2 huffman | 0.3550|3261452|
