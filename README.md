# MessagesCompress
Analysis of compression algorithms. Data - a lot of small messages (~100 bytes long)

results.txt - результаты полученные на базе случайных публичных твитов (ее можно сказать по ссылке https://www.dropbox.com/s/uifh1nrj512phlh/random-twits-ru.txt?dl=0).

### Постановка задачи:
Нужно написать алгоритм, который сжимает которкие сообщения. Вначале ему дается на вход вся база сообщений, на этом этапе алгоритм имеет право предподсчатать какие-нибудь статистики и сохранить их в память.

После этого алгоритму последовательно даются отдельные сообщения, которые он должен сжимать и расжимать пользуясь только предподсчитанными данными. 

В качестве меры эффективность алгоритма считаем величину compress_ratio = (total_additional_size + total_compressed_messages_size) / total_messages_size, где 
* total_additional_size - общее количество памяти, которое алгоритм потратил на хранение дополнительных статистик
* total_compressed_messages_size - суммарный размер сообщений в сжатом виде
* total_messages_size - суммарный размер исходных сообщений

Чем меньше данная величина тем лучше. Если величина равна 1, то сообщения сосвем не сжаты.

### Данные
В качестве исходных данных был взят набор публичных случайных русскоязычных твиттов, который получил с помощью twitter Streaming API. Суммарный размер сообщений 248 мегабайт. Общее количество сообщений - 2089378. Средняя длина - 124 символа (все хранится в кодировке cp1251, поэтому некоторые символы могут занимать больше 1 байта). 

### Алгоритмы

Потом тут будет подробное описание.

* Empty
* One letter huffman
* LZW
* Words huffman
* Words huffman, bytes huffman
* Words huffman, bytes 2 huffman

### 

Результаты:

|Алгоритм|Compression ratio|Размер дополнительной информации (в байтах)|
|-|-|-|
|One letter huffman | 0.7278|1024|
|LZW | 0.5837|327680|
|Words huffman | 0.3755|4792892|
|Words huffman, bytes huffman | 0.3634|3912623|
|Words huffman, bytes 2 huffman | 0.3550|3261452|
